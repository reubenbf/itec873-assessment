{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assessed_task1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "qETonJhsXbuH",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Assessed Task 1"
      ]
    },
    {
      "metadata": {
        "id": "a8RJPBbfXbuI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 1 Dataset"
      ]
    },
    {
      "metadata": {
        "id": "8OCbW-5cXbuJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "In this task, you will have the opportunity to use the `KNeighborsRegressor` to predict a car's market price using its attributes. \n",
        "\n",
        "This data set consists of three types of entities: \n",
        "* the specification of an auto in terms of various characteristics;\n",
        "* its assigned insurance risk rating;\n",
        "* its normalized losses in use as compared to other cars. \n",
        "\n",
        "The second rating corresponds to the degree to which the auto is more risky than its price indicates. Cars are initially assigned a risk factor symbol associated with its price. Then, if it is more risky (or less), this symbol is adjusted by moving it up (or down) the scale. Actuarians call this process \"symboling\". A value of +3 indicates that the auto is risky, -3 that it is probably pretty safe. "
      ]
    },
    {
      "metadata": {
        "id": "EOOwD2CJXbuJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 Attribute Information"
      ]
    },
    {
      "metadata": {
        "id": "j1w037tOXbuK",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "1. symboling: -3, -2, -1, 0, 1, 2, 3. \n",
        "2. normalized-losses: continuous from 65 to 256. \n",
        "3. make: alfa-romero, audi, bmw, chevrolet, dodge, honda,isuzu, jaguar, mazda, mercedes-benz, mercury, mitsubishi, nissan, peugot, plymouth, porsche, renault, saab, subaru, toyota, volkswagen, volvo \n",
        "4. fuel-type: diesel, gas. \n",
        "5. aspiration: std, turbo. \n",
        "6. num-of-doors: four, two. \n",
        "7. body-style: hardtop, wagon, sedan, hatchback, convertible. \n",
        "8. drive-wheels: 4wd, fwd, rwd. \n",
        "9. engine-location: front, rear. \n",
        "10. wheel-base: continuous from 86.6 120.9. \n",
        "11. length: continuous from 141.1 to 208.1. \n",
        "12. width: continuous from 60.3 to 72.3. \n",
        "13. height: continuous from 47.8 to 59.8. \n",
        "14. curb-weight: continuous from 1488 to 4066. \n",
        "15. engine-type: dohc, dohcv, l, ohc, ohcf, ohcv, rotor. \n",
        "16. num-of-cylinders: eight, five, four, six, three, twelve, two. \n",
        "17. engine-size: continuous from 61 to 326. \n",
        "18. fuel-system: 1bbl, 2bbl, 4bbl, idi, mfi, mpfi, spdi, spfi. \n",
        "19. bore: continuous from 2.54 to 3.94. \n",
        "20. stroke: continuous from 2.07 to 4.17. \n",
        "21. compression-ratio: continuous from 7 to 23. \n",
        "22. horsepower: continuous from 48 to 288. \n",
        "23. peak-rpm: continuous from 4150 to 6600. \n",
        "24. city-mpg: continuous from 13 to 49. \n",
        "25. highway-mpg: continuous from 16 to 54. \n",
        "26. price: continuous from 5118 to 45400."
      ]
    },
    {
      "metadata": {
        "id": "5DaFlJ0uXbuL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 3 Relevant Papers"
      ]
    },
    {
      "metadata": {
        "id": "o5x5kU8TXbuM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Kibler, D., Aha, D.W., & Albert,M. (1989). Instance-based prediction of real-valued attributes. Computational Intelligence, Vol 5, 51--57. "
      ]
    },
    {
      "metadata": {
        "id": "LOFfe05WXbuM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 4 Import Data"
      ]
    },
    {
      "metadata": {
        "id": "4ZR-3Jn4XbuN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Q1**: Upload the dataset of `Cars.data`. Import the dataset into a data frame named **cars**. Please also name each attribute (see *Attribute Information* section above)."
      ]
    },
    {
      "metadata": {
        "id": "AytNJyBYWBoD",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(pd.read_csv('Cars.data', sep=\",\", header=None))\n",
        "df.columns = [\"symboling\", \"normalized-losses\", \"make\", \"fule-type\", \"aspiration\", \"num-of-doors\", \"body-style\", \"drive-wheels\", \"engine-location\", \"wheel-base\", \"length\", \"width\",\"height\",\"curb-weight\",\"engine-type\",\"num-of-cylinders\",\"engine-size\",\"fuel-system\",\"bore\",\"stroke\",\"compression-ratio\",\"horsepower\",\"peak-rpm\",\"city-mpg\",\"highway-mpg\",\"price\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ygcoEfZtZITR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Upload the data, import the file and declare it as a dataframe using Pandas. This dataframe is named as **df** .\n",
        "\n",
        "Later, we name each attribute using *pandas.DataFrame.columns* attribute"
      ]
    },
    {
      "metadata": {
        "id": "kOTNGqUZXbuO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Q2**: Select only the columns with continuous values imported into a new data frame named **numeric_cars** and report the first five instances.\n",
        "\n",
        "Note: The first attribute 'symboling' will be treated as a categorical (discrete) variable."
      ]
    },
    {
      "metadata": {
        "id": "jIRfxkZmdTTC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "df = df.replace('?',np.nan)\n",
        "numeric_cars = pd.concat([df[df.columns[df.isnull().any()]], df.select_dtypes(include=[np.number])], axis=1, join='inner').drop(columns=['num-of-doors','symboling'])\n",
        "numeric_cars.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "IzGLBqlZag4Y",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Output](https://i.imgur.com/duAByZX.png)\n",
        "The *Attribute Information* section has information on all the continuous columns, and are therefore *numpy.number* type. However, some continuous columns containing *?* are object type and cannot be selected using *.select_dtypes()*\n",
        "\n",
        "Therefore, we can convert the columns containing *?* with NaN using *.replace()*. Later concat the NaN columns and continuous column using inner join while dropping the *num-of-doors* and *symboling* columns since they aren't continuous and save it into dataframe called **numeric_cars**"
      ]
    },
    {
      "metadata": {
        "id": "09VRhXnZXbuO",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 5 Data Cleaning"
      ]
    },
    {
      "metadata": {
        "id": "loHSofhjXbuP",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Q3**: Replace the \"?\" with \"NaN\" in the **numeric_cars** data.  \n",
        "Hint: Refer to `np.nan`"
      ]
    },
    {
      "metadata": {
        "id": "3P292hF_dC1q",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This has already been done in the previous question "
      ]
    },
    {
      "metadata": {
        "id": "Z_UzUc17XbuQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Q4**: Set **numeric_cars** data type into \"float\" and report the number of null values for each attribute (column)."
      ]
    },
    {
      "metadata": {
        "id": "rKI8JU_1i6ZX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "numeric_cars = numeric_cars.astype(float)\n",
        "numeric_cars.isna().sum(axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "CcGMn8wOdyz9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Output](https://i.imgur.com/ooGcYjy.png)\n",
        "Now that the **numeric_cars** dataframe only has continuous values, you can use the *.astype()* function to convert it to 'float' type.\n",
        "\n",
        "*.isna()* returns a dataframe with *True/False* values, this can be summed up based on each column setting the axis as '0'"
      ]
    },
    {
      "metadata": {
        "id": "gq0gYZlFXbuQ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Q5**: Because `price` is the column to predict, let's remove any rows with missing `price` values, if any. And again report the number of null values for each attribute."
      ]
    },
    {
      "metadata": {
        "id": "zOxb6vxlQUWv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "numeric_cars = numeric_cars.dropna(axis=0, subset=['price'])\n",
        "numeric_cars.isna().sum(axis = 0)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JIAm6YNuhFgz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Output](https://i.imgur.com/vqRo4to.png)\n",
        "To remove required  missing values, the *.dropna()* function can be used by setring the subset to 'price' \n",
        "\n",
        "The number of null values can be reported just as before"
      ]
    },
    {
      "metadata": {
        "id": "LHaIqmP1XbuR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Q6**: Replace missing values in other columns using column means and check that there's no more missing values!"
      ]
    },
    {
      "metadata": {
        "id": "FGmZVQ5QQz2J",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "numeric_cars = numeric_cars.fillna(numeric_cars.mean())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YOX-W9fLiBt4",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "*.fillna()* helps filling up NaN with a specific value, here we replace it with the column mean using *.mean()*. This automatically calculates mean for each column"
      ]
    },
    {
      "metadata": {
        "id": "2CrhYEiPXbuS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Q7**: Normalise all columnns to range from 0 to 1 except for the target column.  \n",
        "Hint: If you want to normalise your data, you can do so as my suggest and calculate the following:  \n",
        "$z_i=(x_i-min(x))/(max(x)-min(x))$  \n",
        "where $x=(x_1,x_2,\\cdots,x_n)$ and $z_i$ is now your $i_{th}$ normalised data."
      ]
    },
    {
      "metadata": {
        "id": "umVaLra_SKIn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "numeric_cars_norm = (numeric_cars.loc[:, numeric_cars.columns != 'price']-numeric_cars.loc[:, numeric_cars.columns != 'price'].min())/(numeric_cars.loc[:, numeric_cars.columns != 'price'].max()-numeric_cars.loc[:, numeric_cars.columns != 'price'].min())\n",
        "numeric_cars[numeric_cars_norm.columns] = numeric_cars_norm"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "iCLqD1xXkEPJ",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The 'price' column can be excluded using the *.loc* attribute, we can normalize the data using the given formula by using the *.min()* and *.max()*  as needed. "
      ]
    },
    {
      "metadata": {
        "id": "LKo-ai-oXbuS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## 6 Univariate Model [KNeighborsRegressor](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html) with Fixed k Value"
      ]
    },
    {
      "metadata": {
        "id": "l3-cmJGVXbuT",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "rmse_results = {}\n",
        "train_cols = numeric_cars.columns.drop('price')\n",
        "train_cols"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G279NL1xAZgI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Output](https://i.imgur.com/k2j3Juh.png)"
      ]
    },
    {
      "metadata": {
        "id": "YxOhXZdaXbuW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Q8**: For each column (minus `price`), train a model, return RMSE value and add to the dictionary `rmse_results`. How to calculate RMSE in python, refer to the [link]( https://stackoverflow.com/questions/45173451/scikit-learn-how-to-calculate-root-mean-square-error-rmse-in-percentage). You require to use `sklearn.metrics import mean_squared_error` and `np.sqrt`.\n",
        "\n",
        "Note: \n",
        "1. Randomise order of rows in data frame before splitting data into training and test set. Set the random seed to 1.\n",
        "2. The first 80% of the data are used as training set and the rest are as test set.\n",
        "3. Fit a KNN model using default k value\n",
        "4. Report the rmse_results under an ascending order (RMSE) with the following format:  \n",
        "`horsepower    8000.0000`   \n",
        "`highway-mpg   9000.0000`\n"
      ]
    },
    {
      "metadata": {
        "id": "zYMIPTDM9tma",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from collections import OrderedDict\n",
        "\n",
        "numeric_cars_rand = numeric_cars.sample(n = numeric_cars.shape[0],random_state=1)\n",
        "X = pd.DataFrame(numeric_cars_rand[train_cols])        \n",
        "y = pd.DataFrame(numeric_cars_rand['price'])\n",
        "X_train, X_test, y_train, y_test = train_test_split(X , y, test_size = 0.2, random_state=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Ac273x_ClS2R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "The rows in **numeric_cars** can be randomized using the *.sample* function setting the 'n' value to all the rows in the dataframe. \n",
        "\n",
        "Now we divide the dataframe into input variables **X** (every column except price) and output variables **y** (containing the 'price' column) \n",
        "\n",
        "These variables can now be divided to their respective training and testing sets using the *train_test_split()* function present in the *model_selection* package while setting the 'test_size' to 0.2 (20%)\n"
      ]
    },
    {
      "metadata": {
        "id": "oYe83rhtfPSv",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for column in X_test:\n",
        "  reg = KNeighborsRegressor().fit(X_train[[column]],y_train)\n",
        "  y_pred = pd.DataFrame(reg.predict(X_test[[column]]))\n",
        "  rmse_results[column] = np.sqrt(mean_squared_error(y_test,y_pred))\n",
        "  \n",
        "rmse_results = OrderedDict(sorted(rmse_results.items(), key=lambda t: t[1]))\n",
        "for column in rmse_results:\n",
        "  print(\"{} {:.4f}\".format(column,round(rmse_results[column])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "j6SD3FVv1t_V",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "![Output](https://i.imgur.com/IqDvkzE.png)\n",
        "Now fit the training sets using the *KNeighborsRegressor().fit()* function making sure to import *.neighbours* package. \n",
        "\n",
        "Predict the output value using the *.predict()* function and compare them with the test values to get their mean square error using *mean_squared_error()* present in the *.metrics* package, later calculate the root mean square using *numpy.sqrt*\n",
        "\n",
        "Sort the **rmse_results** using *collections.OrderedDict()* and print the dictionary"
      ]
    }
  ]
}